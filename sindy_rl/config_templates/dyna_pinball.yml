# --------------------------------------------------------
# --------------------------------------------------------

exp_dir: pinball_new
n_train_iter: 40000
dyn_fit_freq: 25
fcnet_hiddens: [64, 64]
use_pbt: False

# used for rolling out new on-policy data for 
# dynamics fitting
real_env:
  class: HydroSVDWrapper
  config: 
    flow: 'Pinball'
    dt: 1.0e-2
    max_steps: 2000
    n_svd: 10
    svd_load: /replace/with/path/to/svd.pkl
    flow_config:
      mesh: 'medium'
      Re: 100
      'observation_type': velocity_probes
      restart: 
        - /replace/with/path/to/checkpoint.ckpt

drl:
  class: PPO
  config:
    training:
      gamma: 0.99
      lr: 3.0e-4
      lambda_: 0.95
      vf_loss_coeff: 0.5
      vf_clip_param: 0.2
      clip_param: 0.2
      grad_clip: 0.5
    environment:
      env: 
      env_config:
        init_real_on_start: False
        reset_from_buffer: True
        max_episode_steps: 2000
        real_env_class: HydroSVDWrapper
        real_env_config: 
          flow: 'Pinball'
          dt: 1.0e-2
          n_svd: 10
          svd_load: /replace/with/path/to/svd
          flow_config:
            mesh: 'medium'
            Re: 100
            'observation_type': velocity_probes
            restart: 
              - /replace/with/path/to/checkpoint.ckpt

        ensemble_modes: 
          dyn: median
          rew: median
        init_weights: True
        act_dim: 3
        obs_dim: 10
        act_bounds: 
          - [-1.0, 1.0]
          - [-1.0, 1.0]
          - [-1.0, 1.0]
        obs_bounds: 
          - [-20, 20]
          - [-20, 20]
          - [-20, 20]
          - [-20, 20]
          - [-20, 20]
          - [-20, 20]
          - [-20, 20]
          - [-20, 20]
          - [-20, 20]
          - [-20, 20]
    framework: torch
    evaluation: 
      evaluation_interval: # None


off_policy_buffer:
  config:
    max_traj:
    max_samples:
  init: 
    type: file
    kwargs: 
      fname: /replace/with/path/to/trajectories.pkl



on_policy_buffer:
  config:
    max_traj: 
    max_samples: 12000
  collect:
    n_steps: 1000
    n_steps_reset: 1000


dynamics_model:
  class: EnsembleSINDyDynamicsModel
  config:
    'dt': 1
    'discrete': True 
    'optimizer': 
      'base_optimizer':
        'name': 'STLSQ'
        'kwargs': 
          'alpha':   1.0e-5
          'threshold': 1.0e-3
      'ensemble':
        'bagging': True
        'library_ensemble': True
        'n_models': 20
        'n_candidates_to_drop': 1
    'feature_library': 
      name: affine
      kwargs:
        poly_deg: 2
        n_state: 10
        n_control: 3
        poly_int: True
        tensor: False

rew_model:
  class: EnsembleSparseRewardModel
  config:
    'use_control': True
    'optimizer': 
        'base_optimizer':
            'name': 'STLSQ'
            'kwargs': 
                'alpha':   1.0e-5
                'threshold': 5.0e-1
        'ensemble':
            'bagging': True
            'library_ensemble': True
            'n_models': 20
    'feature_library': 
        name: PolynomialLibrary
        kwargs:
            degree: 2
            include_bias: True
            include_interaction: True


ray_config:
    run_config:
        name: dyna_pinball
        stop:
            num_env_steps_sampled: 5.0e+6
        log_to_file: True 
    tune_config:
        num_samples: 20 # number of sample trials to run
    checkpoint_freq: 10